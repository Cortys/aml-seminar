% Encoding: UTF-8

@InProceedings{Domhan2015,
  author    = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
  title     = {Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves},
  booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
  year      = {2015},
  series    = {IJCAI'15},
  publisher = {AAAI Press},
  location  = {Buenos Aires, Argentina},
  isbn      = {978-1-57735-738-4},
  pages     = {3460--3468},
  url       = {http://dl.acm.org/citation.cfm?id=2832581.2832731},
  acmid     = {2832731},
  numpages  = {9},
}

@Article{Kleiner2011,
  author      = {Ariel Kleiner and Ameet Talwalkar and Purnamrita Sarkar and Michael I. Jordan},
  title       = {A Scalable Bootstrap for Massive Data},
  journal     = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year        = {2014},
  volume      = {76},
  number      = {4},
  month       = {mar},
  pages       = {795--816},
  doi         = {10.1111/rssb.12050},
  eprint      = {1112.5016v2},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets---which are increasingly prevalent---the computation of bootstrap-based quantities can be prohibitively demanding computationally. While variants such as subsampling and the $m$ out of $n$ bootstrap can be used in principle to reduce the cost of bootstrap computations, we find that these methods are generally not robust to specification of hyperparameters (such as the number of subsampled data points), and they often require use of more prior information (such as rates of convergence of estimators) than the bootstrap. As an alternative, we introduce the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to yield a robust, computationally efficient means of assessing the quality of estimators. BLB is well suited to modern parallel and distributed computing architectures and furthermore retains the generic applicability and statistical efficiency of the bootstrap. We demonstrate BLB's favorable statistical performance via a theoretical analysis elucidating the procedure's properties, as well as a simulation study comparing BLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. In addition, we present results from a large-scale distributed implementation of BLB demonstrating its computational superiority on massive data, a method for adaptively selecting BLB's hyperparameters, an empirical study applying BLB to several real datasets, and an extension of BLB to time series data.},
  file        = {:http\://arxiv.org/pdf/1112.5016v2:PDF},
  keywords    = {stat.ME, stat.CO, stat.ML},
  publisher   = {Wiley},
}

@Article{Byrd2012,
  author    = {Richard H. Byrd and Gillian M. Chin and Jorge Nocedal and Yuchen Wu},
  title     = {Sample size selection in optimization methods for machine learning},
  journal   = {Mathematical Programming},
  year      = {2012},
  volume    = {134},
  number    = {1},
  month     = {jun},
  pages     = {127--155},
  doi       = {10.1007/s10107-012-0572-5},
  publisher = {Springer Nature},
}

@Article{Wang2017,
  author      = {HaiYing Wang and Rong Zhu and Ping Ma},
  title       = {Optimal Subsampling for Large Sample Logistic Regression},
  journal     = {Journal of the American Statistical Association},
  year        = {2018},
  volume      = {113},
  number      = {522},
  month       = {apr},
  pages       = {829--844},
  doi         = {10.1080/01621459.2017.1292914},
  eprint      = {1702.01166v2},
  eprintclass = {stat.CO},
  eprinttype  = {arXiv},
  abstract    = {For massive data, the family of subsampling algorithms is popular to downsize the data volume and reduce computational burden. Existing studies focus on approximating the ordinary least squares estimate in linear regression, where statistical leverage scores are often used to define subsampling probabilities. In this paper, we propose fast subsampling algorithms to efficiently approximate the maximum likelihood estimate in logistic regression. We first establish consistency and asymptotic normality of the estimator from a general subsampling algorithm, and then derive optimal subsampling probabilities that minimize the asymptotic mean squared error of the resultant estimator. An alternative minimization criterion is also proposed to further reduce the computational cost. The optimal subsampling probabilities depend on the full data estimate, so we develop a two-step algorithm to approximate the optimal subsampling procedure. This algorithm is computationally efficient and has a significant reduction in computing time compared to the full data approach. Consistency and asymptotic normality of the estimator from a two-step algorithm are also established. Synthetic and real data sets are used to evaluate the practical performance of the proposed method.},
  file        = {:http\://arxiv.org/pdf/1702.01166v2:PDF},
  keywords    = {stat.CO, stat.ME, stat.ML},
  publisher   = {Informa {UK} Limited},
}

@Article{Al-Jarrah2015,
  author      = {Al-Jarrah, O. Y. and Yoo, P. D. and Muhaidat, S. and Karagiannidis, G. K. and Taha, K.},
  title       = {Efficient Machine Learning for Big Data: A Review},
  journal     = {Big Data Research},
  year        = {2015},
  volume      = {2},
  number      = {3},
  month       = {sep},
  pages       = {87--93},
  doi         = {10.1016/j.bdr.2015.04.001},
  eprint      = {1503.05296v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years, in fact, as much as 90% of current data were created in the last couple of years,a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven, the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.},
  file        = {:http\://arxiv.org/pdf/1503.05296v1:PDF},
  keywords    = {cs.LG, cs.AI},
  publisher   = {Elsevier {BV}},
}

@InProceedings{Almeida2000,
  author    = {M. Barros de Almeida and A. de Padua Braga and J. P. Braga},
  title     = {{SVM}-KM: speeding SVMs learning with a priori cluster selection and k-means},
  booktitle = {Proc. Vol.1. Sixth Brazilian Symp. Neural Networks},
  year      = {2000},
  month     = nov,
  pages     = {162--167},
  doi       = {10.1109/SBRN.2000.889732},
  issn      = {1522-4899},
  keywords  = {learning automata, learning (artificial intelligence), generalisation (artificial intelligence), pattern clustering, pattern classification, SVM-KM, cluster selection, k-means, support vector machines, optimization phase, training vectors, separation margins, class label, cluster centers, training time, generalization capability, Support vector machines, Pattern recognition, Quadratic programming, Text recognition, Kernel, Lagrangian functions, Acceleration, Process design, Unsolicited electronic mail, Gene expression},
}

@Article{Fithian2013,
  author       = {William Fithian and Trevor Hastie},
  title        = {Local case-control sampling: Efficient subsampling in imbalanced data sets},
  journaltitle = {Annals of Statistics 2014, Vol. 42, No. 5, 1693-1724},
  date         = {2013-06-16},
  doi          = {10.1214/14-AOS1220},
  eprint       = {1306.3706v2},
  eprintclass  = {stat.CO},
  eprinttype   = {arXiv},
  abstract     = {For classification problems with significant class imbalance, subsampling can reduce computational costs at the price of inflated variance in estimating model parameters. We propose a method for subsampling efficiently for logistic regression by adjusting the class balance locally in feature space via an accept-reject scheme. Our method generalizes standard case-control sampling, using a pilot estimate to preferentially select examples whose responses are conditionally rare given their features. The biased subsampling is corrected by a post-hoc analytic adjustment to the parameters. The method is simple and requires one parallelizable scan over the full data set. Standard case-control sampling is inconsistent under model misspecification for the population risk-minimizing coefficients $\theta^*$. By contrast, our estimator is consistent for $\theta^*$ provided that the pilot estimate is. Moreover, under correct specification and with a consistent, independent pilot estimate, our estimator has exactly twice the asymptotic variance of the full-sample MLE - even if the selected subsample comprises a miniscule fraction of the full data set, as happens when the original data are severely imbalanced. The factor of two improves to $1+\frac{1}{c}$ if we multiply the baseline acceptance probabilities by $c>1$ (and weight points with acceptance probability greater than 1), taking roughly $\frac{1+c}{2}$ times as many data points into the subsample. Experiments on simulated and real data show that our method can substantially outperform standard case-control subsampling.},
  file         = {:http\://arxiv.org/pdf/1306.3706v2:PDF},
  keywords     = {stat.CO, stat.ML},
}

@Article{Bang2014,
  author    = {Sungwan Bang and Myoungshic Jhun},
  title     = {Weighted Support Vector Machine Using k-Means Clustering},
  journal   = {Communications in Statistics - Simulation and Computation},
  year      = {2014},
  volume    = {43},
  number    = {10},
  month     = {jun},
  pages     = {2307--2324},
  doi       = {10.1080/03610918.2012.762388},
  publisher = {Informa {UK} Limited},
}

@Article{Wang2005,
  author    = {Jiaqi Wang and Xindong Wu and Chengqi Zhang},
  title     = {Support vector machines based on K-means clustering for real-time business intelligence systems},
  journal   = {International Journal of Business Intelligence and Data Mining},
  year      = {2005},
  volume    = {1},
  number    = {1},
  pages     = {54},
  doi       = {10.1504/ijbidm.2005.007318},
  publisher = {Inderscience Publishers},
}

@Article{Lee2007,
  author                     = {Lee, S. J. and Park, C. and Jhun, M. and Ko, J-Y.},
  title                      = {{Support vector machine using K-means clustering}},
  journal                    = {{Journal of the Korean Statistical Society}},
  year                       = {{2007}},
  language                   = {{English}},
  volume                     = {{36}},
  number                     = {{1}},
  month                      = {{MAR}},
  pages                      = {{175-182}},
  issn                       = {{1226-3192}},
  abstract                   = {{The support vector machine has been successful in many applications
   because of its flexibility and high accuracy. However, when a training
   data set is large or imbalanced, the support vector machine may suffer
   from significant computational problem or loss of accuracy in predicting
   minority classes. We propose a modified version of the support vector
   machine using the K-means clustering that exploits the information in
   class labels during the clustering process. For large data sets, our
   method can save the computation time by reducing the number of data
   points without significant loss of accuracy. Moreover, our method can
   deal with imbalanced data sets effectively by alleviating the influence
   of dominant class.}},
  address                    = {{SCIENCE \& TECHNOLOGY, BLDG RM 709, 635-4 YEOGSAM-DONG, KANGNAM-GU, SEOUL, 135-703, SOUTH KOREA}},
  affiliation                = {{Lee, SJ (Reprint Author), Seoul Natl Univ, Dept Stat, Seoul 151747, South Korea. Lee, S. J., Seoul Natl Univ, Dept Stat, Seoul 151747, South Korea. Park, C., Korea Univ, Inst Stat, Seoul 136701, South Korea. Jhun, M.; Ko, J-Y., Korea Univ, Dept Stat, Seoul 136701, South Korea.}},
  author-email               = {{seaphant@statcom.snu.ac.kr}},
  doc-delivery-number        = {{287UH}},
  journal-iso                = {{J. Korean Stat. Soc.}},
  keywords                   = {{class imbalance; K-means clustering; support vector machine}},
  number-of-cited-references = {{8}},
  publisher                  = {{KOREAN STATISTICAL SOC}},
  research-areas             = {{Mathematics}},
  times-cited                = {{3}},
  type                       = {{Article}},
  unique-id                  = {{ISI:000254941500010}},
  usage-count-last-180-days  = {{0}},
  usage-count-since-2013     = {{1}},
  web-of-science-categories  = {{Statistics \& Probability}},
}

@InProceedings{Norazan2009,
  author       = {Norazan, MR and Habshah, M and Imon, AHMR and Chen, Shengyong},
  title        = {Weighted bootstrap with probability in regression},
  booktitle    = {WSEAS International Conference. Proceedings. Mathematics and Computers in Science and Engineering},
  year         = {2009},
  number       = {8},
  organization = {World Scientific and Engineering Academy and Society},
}

@InProceedings{Swersky2013,
  author    = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P.},
  title     = {Multi-task Bayesian Optimization},
  booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
  year      = {2013},
  series    = {NIPS'13},
  publisher = {Curran Associates Inc.},
  location  = {Lake Tahoe, Nevada},
  pages     = {2004--2012},
  url       = {http://dl.acm.org/citation.cfm?id=2999792.2999836},
  acmid     = {2999836},
  address   = {USA},
  numpages  = {9},
}

@Online{Wang2018,
  author      = {HaiYing Wang},
  title       = {More Efficient Estimation for Logistic Regression with Optimal Subsample},
  year        = {2018},
  date        = {2018-02-08},
  abstract    = {Facing large amounts of data, subsampling is a practical technique to extract useful information. For this purpose, Wang et al. (2017) developed an Optimal Subsampling Method under the A-optimality Criterion (OSMAC) for logistic regression that samples more informative data points with higher probabilities. However, the original OSMAC estimator use inverse of optimal subsampling probabilities as weights in the likelihood function. This reduces contributions of more informative data points and the resultant estimator may lose efficiency. In this paper, we propose a more efficient estimator based on OSMAC subsample without weighting the likelihood function. Both asymptotic results and numerical results show that the new estimator is more efficient. In addition, our focus in this paper is inference for the true parameter, while Wang et al. (2017) focuses on approximating the full data estimator. We also develop a new algorithm based on Poisson sampling, which does not require to approximate the optimal subsampling probabilities all at once. This is computationally advantageous when available random-access memory is not enough to hold the full data. Interestingly, asymptotic distributions also show that Poisson sampling produces more efficient estimator if the sampling rate, the ratio of the subsample size to the full data sample size, does not converge to zero. We also obtain the unconditional asymptotic distribution for the estimator based on Poisson sampling.},
  eprint      = {1802.02698v2},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1802.02698v2:PDF},
  keywords    = {stat.ME},
}

@Article{Shahriari2016,
  author    = {Bobak Shahriari and Kevin Swersky and Ziyu Wang and Ryan P. Adams and Nando de Freitas},
  title     = {Taking the Human Out of the Loop: A Review of Bayesian Optimization},
  journal   = {Proceedings of the {IEEE}},
  year      = {2016},
  volume    = {104},
  number    = {1},
  month     = {jan},
  pages     = {148--175},
  doi       = {10.1109/jproc.2015.2494218},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InCollection{Schoen2017,
  author    = {Stephen Schön and Gael Kermarrec and Boris Kargoll and Ingo Neumann and Olga Kosheleva and Vladik Kreinovich},
  title     = {Why Student Distributions? Why Matern's Covariance Model? A Symmetry-Based Explanation},
  booktitle = {Econometrics for Financial Applications},
  year      = {2017},
  publisher = {Springer International Publishing},
  pages     = {266--275},
  doi       = {10.1007/978-3-319-73150-6_21},
  month     = {dec},
}

@InProceedings{Klein2017,
  author      = {Aaron Klein and Stefan Falkner and Simon Bartels and Philipp Hennig and Frank Hutter},
  title       = {Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets},
  booktitle   = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  year        = {2017},
  editor      = {Aarti Singh and Jerry Zhu},
  volume      = {54},
  series      = {Proceedings of Machine Learning Research},
  publisher   = {PMLR},
  month       = {20--22 Apr},
  pages       = {528--536},
  eprint      = {1605.07079v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  url         = {http://proceedings.mlr.press/v54/klein17a.html},
  abstract    = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed Fabolas, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that Fabolas often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.},
  address     = {Fort Lauderdale, FL, USA},
  file        = {:http\://arxiv.org/pdf/1605.07079v2:PDF},
  keywords    = {cs.LG, cs.AI, stat.ML},
}

@Misc{Bickel1997,
  author        = {Bickel, P. J. and Götze, F. and van Zwet, W. R.},
  title         = {Resampling fewer than n observations: gains, losses, and remedies for losses},
  year          = {1997},
  __markedentry = {[clemens:6]},
  abstract      = {this paper we},
}

@Comment{jabref-meta: databaseType:biblatex;}
